"""
æ¶ˆæ¯ç”Ÿæˆå™¨

è´Ÿè´£ä½¿ç”¨LLMç”Ÿæˆä¸»åŠ¨æ¶ˆæ¯å¹¶å¤„ç†æ¶ˆæ¯å‘é€
"""

import asyncio
import re
from datetime import datetime
from astrbot.api import logger
from astrbot.api.event import MessageChain

from ..core.runtime_data import runtime_data
from .ai_schedule_analyzer import analyze_for_schedule


class MessageGenerator:
    """æ¶ˆæ¯ç”Ÿæˆå™¨ç±»"""

    # åˆ†å‰²æ¨¡å¼æ­£åˆ™è¡¨è¾¾å¼
    SPLIT_MODE_PATTERNS = {
        "backslash": r"\\",
        "newline": r"\n",
        "comma": r",",
        "semicolon": r";",
        "punctuation": r"[,;ã€‚!?]",
    }

    def __init__(
        self,
        config: dict,
        context,
        prompt_builder,
        conversation_manager,
        user_info_manager,
    ):
        """åˆå§‹åŒ–æ¶ˆæ¯ç”Ÿæˆå™¨

        Args:
            config: é…ç½®å­—å…¸
            context: AstrBotä¸Šä¸‹æ–‡å¯¹è±¡
            prompt_builder: æç¤ºè¯æ„å»ºå™¨
            conversation_manager: ä¼šè¯ç®¡ç†å™¨
            user_info_manager: ç”¨æˆ·ä¿¡æ¯ç®¡ç†å™¨
        """
        self.config = config
        self.context = context
        self.prompt_builder = prompt_builder
        self.conversation_manager = conversation_manager
        self.user_info_manager = user_info_manager

    def get_llm_provider(self):
        """è·å–LLMæä¾›å•†

        Returns:
            LLMæä¾›å•†å¯¹è±¡ï¼Œå¤±è´¥è¿”å›None
        """
        provider = self.context.get_using_provider()
        if not provider:
            logger.warning("LLMæä¾›å•†ä¸å¯ç”¨ï¼Œæ— æ³•ç”Ÿæˆä¸»åŠ¨æ¶ˆæ¯")
        return provider

    def is_duplicate_message(self, session: str, message: str) -> bool:
        """æ£€æµ‹æ¶ˆæ¯æ˜¯å¦ä¸ä¸Šæ¬¡å‘é€çš„é‡å¤

        Args:
            session: ä¼šè¯ID
            message: å¾…æ£€æµ‹çš„æ¶ˆæ¯

        Returns:
            True å¦‚æœé‡å¤ï¼ŒFalse å¦‚æœä¸é‡å¤
        """
        last_message = runtime_data.session_last_proactive_message.get(session)
        if not last_message:
            return False

        # å®Œå…¨ç›¸åŒ
        if message == last_message:
            logger.debug("é‡å¤æ£€æµ‹: æ¶ˆæ¯ä¸ä¸Šæ¬¡å®Œå…¨ç›¸åŒ")
            return True

        # å‰50ä¸ªå­—ç¬¦ç›¸åŒï¼ˆé¿å…ä»…ç»“å°¾ç•¥æœ‰ä¸åŒçš„æƒ…å†µï¼‰
        check_length = 50
        if len(message) >= check_length and len(last_message) >= check_length:
            if message[:check_length] == last_message[:check_length]:
                logger.debug("é‡å¤æ£€æµ‹: æ¶ˆæ¯å‰50å­—ç¬¦ä¸ä¸Šæ¬¡ç›¸åŒ")
                return True

        return False

    def record_last_message(self, session: str, message: str):
        """è®°å½•ä¼šè¯æœ€åå‘é€çš„ä¸»åŠ¨æ¶ˆæ¯

        Args:
            session: ä¼šè¯ID
            message: å‘é€çš„æ¶ˆæ¯
        """
        runtime_data.session_last_proactive_message[session] = message

    async def generate_proactive_message_with_retry(
        self, session: str, max_retries: int = 3, override_prompt: str = None
    ) -> tuple:
        """ç”Ÿæˆä¸»åŠ¨æ¶ˆæ¯ï¼Œå¸¦é‡å¤æ£€æµ‹å’Œé‡è¯•

        Args:
            session: ä¼šè¯ID
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°

        Returns:
            å…ƒç»„ (ç”Ÿæˆçš„æ¶ˆæ¯, ä½¿ç”¨çš„ä¸»åŠ¨å¯¹è¯æç¤ºè¯)ï¼Œå¤±è´¥è¿”å› (None, None)
        """
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨é‡å¤æ£€æµ‹
        proactive_config = self.config.get("proactive_reply", {})
        duplicate_detection_enabled = proactive_config.get(
            "duplicate_detection_enabled", True
        )

        message = None
        final_prompt = None
        for attempt in range(max_retries + 1):
            message, final_prompt = await self.generate_proactive_message(
                session, override_prompt
            )
            if not message:
                return None, None

            # å¦‚æœæœªå¯ç”¨é‡å¤æ£€æµ‹ï¼Œç›´æ¥è¿”å›
            if not duplicate_detection_enabled:
                return message, final_prompt

            # æ£€æµ‹é‡å¤
            if not self.is_duplicate_message(session, message):
                return message, final_prompt

            # é‡å¤äº†ï¼Œè®°å½•æ—¥å¿—
            if attempt < max_retries:
                logger.warning(
                    f"ğŸ”„ æ£€æµ‹åˆ°é‡å¤æ¶ˆæ¯ï¼Œé‡æ–°ç”Ÿæˆ ({attempt + 1}/{max_retries})"
                )
            else:
                logger.warning("âš ï¸ å¤šæ¬¡é‡è¯•åä»ä¸ºé‡å¤æ¶ˆæ¯ï¼Œä½¿ç”¨å½“å‰æ¶ˆæ¯")

        return message, final_prompt

    async def generate_proactive_message(
        self, session: str, override_prompt: str = None
    ) -> tuple:
        """ä½¿ç”¨LLMç”Ÿæˆä¸»åŠ¨æ¶ˆæ¯å†…å®¹

        Args:
            session: ä¼šè¯ID

        Returns:
            å…ƒç»„ (ç”Ÿæˆçš„æ¶ˆæ¯, ä½¿ç”¨çš„ä¸»åŠ¨å¯¹è¯æç¤ºè¯)ï¼Œå¤±è´¥è¿”å› (None, None)
        """
        try:
            # æ£€æŸ¥LLMæ˜¯å¦å¯ç”¨
            provider = self.get_llm_provider()
            if not provider:
                return None, None

            # è·å–å¹¶å¤„ç†ä¸»åŠ¨å¯¹è¯æç¤ºè¯
            if override_prompt:
                final_prompt = override_prompt
                # ç®€å•æ›¿æ¢å ä½ç¬¦ï¼ˆä¿æŒä¸€è‡´æ€§ï¼‰
                final_prompt = self.prompt_builder.replace_placeholders(
                    final_prompt,
                    session,
                    self.config,
                    self.user_info_manager.build_user_context_for_proactive,
                )
            else:
                final_prompt = self.prompt_builder.get_proactive_prompt(
                    session, self.user_info_manager.build_user_context_for_proactive
                )

            if not final_prompt:
                return None, None

            # è·å–äººæ ¼ç³»ç»Ÿæç¤ºè¯
            base_system_prompt = await self.prompt_builder.get_persona_system_prompt(
                session
            )

            # è·å–å†å²è®°å½•ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            contexts = []
            proactive_config = self.config.get("proactive_reply", {})

            if proactive_config.get("include_history_enabled", False):
                history_count = proactive_config.get("history_message_count", 10)
                history_count = max(1, min(50, history_count))
                contexts = await self.conversation_manager.get_conversation_history(
                    session, history_count
                )
                # è®°å½•å†å²è®°å½•è·å–ç»“æœï¼ˆä½¿ç”¨ info çº§åˆ«ç¡®ä¿å¯è§ï¼‰
                logger.info(f"ğŸ“š ä¸»åŠ¨æ¶ˆæ¯ç”Ÿæˆ: è·å–åˆ° {len(contexts)} æ¡å†å²è®°å½•")
                if contexts:
                    last_msg = contexts[-1]
                    content_preview = last_msg.get("content", "")[:80]
                    logger.info(
                        f"ğŸ“ æœ€åä¸€æ¡å†å²: [{last_msg.get('role')}] {content_preview}"
                    )
            else:
                logger.info("ğŸ“š ä¸»åŠ¨æ¶ˆæ¯ç”Ÿæˆ: å†å²è®°å½•åŠŸèƒ½æœªå¯ç”¨")

            # æ„å»ºå†å²è®°å½•å¼•å¯¼æç¤ºè¯
            history_guidance = ""
            if proactive_config.get("include_history_enabled", False) and contexts:
                history_guidance = "\n\n--- ä¸Šä¸‹æ–‡è¯´æ˜ ---\nä½ å¯ä»¥å‚è€ƒä¸Šè¿°å¯¹è¯å†å²æ¥ç”Ÿæˆæ›´è‡ªç„¶å’Œè¿è´¯çš„å›å¤ã€‚"

            # æ„å»ºç»„åˆç³»ç»Ÿæç¤ºè¯
            combined_system_prompt = self.prompt_builder.build_combined_system_prompt(
                base_system_prompt,
                final_prompt,
                history_guidance,
                session,
                self.user_info_manager.build_user_context_for_proactive,
            )

            # è°ƒç”¨LLMç”Ÿæˆä¸»åŠ¨æ¶ˆæ¯
            logger.debug(f"è°ƒç”¨LLMç”Ÿæˆä¸»åŠ¨æ¶ˆæ¯, contextsæ•°é‡: {len(contexts)}")
            llm_response = await provider.text_chat(
                prompt="[è¯·æ ¹æ®ä¸Šè¿°æŒ‡ä»¤ç”Ÿæˆå›å¤]",
                session_id=None,
                contexts=contexts,
                image_urls=[],
                func_tool=None,
                system_prompt=combined_system_prompt,
            )

            if llm_response and llm_response.role == "assistant":
                generated_message = llm_response.completion_text
                if generated_message:
                    generated_message = generated_message.strip()
                    logger.info("LLMç”Ÿæˆä¸»åŠ¨æ¶ˆæ¯æˆåŠŸ")
                    return generated_message, final_prompt
                else:
                    logger.warning("LLMè¿”å›äº†ç©ºæ¶ˆæ¯")
                    return None, None
            else:
                logger.warning(f"LLMå“åº”å¼‚å¸¸: {llm_response}")
                return None, None

        except Exception as e:
            logger.error(f"ä½¿ç”¨LLMç”Ÿæˆä¸»åŠ¨æ¶ˆæ¯å¤±è´¥: {e}")
            import traceback

            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            raise

    async def send_proactive_message(
        self, session: str, override_prompt: str = None
    ) -> dict | None:
        """å‘æŒ‡å®šä¼šè¯å‘é€ä¸»åŠ¨æ¶ˆæ¯

        Args:
            session: ä¼šè¯ID

        Returns:
            AI è‡ªä¸»è°ƒåº¦ä¿¡æ¯ {"delay_minutes": int, "follow_up_prompt": str, "fire_time": str}
            æˆ– Noneï¼ˆæ— è°ƒåº¦ï¼‰

        Raises:
            RuntimeError: æ¶ˆæ¯ç”Ÿæˆå¤±è´¥æ—¶æŠ›å‡º
            Exception: å‘é€è¿‡ç¨‹ä¸­çš„å…¶ä»–å¼‚å¸¸ä¼šå‘ä¸Šä¼ æ’­
        """
        try:
            # ä½¿ç”¨å¸¦é‡å¤æ£€æµ‹çš„LLMç”Ÿæˆä¸»åŠ¨æ¶ˆæ¯
            (
                message,
                proactive_prompt_used,
            ) = await self.generate_proactive_message_with_retry(
                session, override_prompt=override_prompt
            )

            if not message:
                raise RuntimeError(f"æ— æ³•ä¸ºä¼šè¯ {session} ç”Ÿæˆä¸»åŠ¨æ¶ˆæ¯")

            original_message = message  # ä¿å­˜åŸå§‹æ¶ˆæ¯ç”¨äºå†å²è®°å½•

            # è®°å½•æœ¬æ¬¡å‘é€çš„æ¶ˆæ¯ï¼ˆç”¨äºä¸‹æ¬¡é‡å¤æ£€æµ‹ï¼‰
            self.record_last_message(session, original_message)

            # å¤„ç†æ¶ˆæ¯åˆ†å‰²å’Œå‘é€
            await self._send_message_with_split(
                session, message, original_message, proactive_prompt_used
            )

            # AI è‡ªä¸»è°ƒåº¦åˆ†æï¼ˆå‘é€åå¼‚æ­¥æ‰§è¡Œï¼Œä¸å½±å“å‘é€æœ¬èº«ï¼‰
            schedule_result = await self.analyze_message_for_schedule(
                session, original_message
            )
            return schedule_result

        except Exception as e:
            logger.error(f"âŒ å‘ä¼šè¯ {session} å‘é€ä¸»åŠ¨æ¶ˆæ¯æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise

    async def analyze_message_for_schedule(
        self, session: str, message: str
    ) -> dict | None:
        """åˆ†æ AI æ¶ˆæ¯æ˜¯å¦åŒ…å«æ—¶é—´çº¦å®šï¼Œå‘èµ·äºŒæ¬¡ LLM è°ƒç”¨

        Args:
            session: ä¼šè¯ID
            message: AI ç”Ÿæˆçš„æ¶ˆæ¯

        Returns:
            è°ƒåº¦ä¿¡æ¯ dict æˆ– None
        """
        ai_schedule_config = self.config.get("ai_schedule", {})
        if not ai_schedule_config.get("enabled", False):
            return None

        provider = self.get_llm_provider()
        if not provider:
            return None

        # è·å–å¯¹è¯å†å²ä½œä¸ºåˆ†æä¸Šä¸‹æ–‡
        proactive_config = self.config.get("proactive_reply", {})
        contexts = []
        if proactive_config.get("include_history_enabled", False):
            history_count = proactive_config.get("history_message_count", 10)
            history_count = max(1, min(50, history_count))
            contexts = await self.conversation_manager.get_conversation_history(
                session, history_count
            )

        # è·å–è‡ªå®šä¹‰åˆ†ææç¤ºè¯
        analysis_prompt = ai_schedule_config.get("analysis_prompt", "")

        # å½“å‰æ—¶é—´
        time_format = self.config.get("user_info", {}).get(
            "time_format", "%Y-%m-%d %H:%M:%S"
        )
        current_time_str = datetime.now().strftime(time_format)

        return await analyze_for_schedule(
            provider=provider,
            ai_message=message,
            contexts=contexts,
            analysis_prompt=analysis_prompt,
            current_time_str=current_time_str,
        )

    async def _send_message_with_split(
        self,
        session: str,
        message: str,
        original_message: str,
        proactive_prompt_used: str = None,
    ):
        """å¤„ç†æ¶ˆæ¯åˆ†å‰²å’Œå‘é€

        Args:
            session: ä¼šè¯ID
            message: å¾…å‘é€çš„æ¶ˆæ¯
            original_message: åŸå§‹æ¶ˆæ¯ï¼ˆç”¨äºå†å²è®°å½•ï¼‰
            proactive_prompt_used: æœ¬æ¬¡ä½¿ç”¨çš„ä¸»åŠ¨å¯¹è¯æç¤ºè¯
        """
        try:
            split_config = self.config.get("message_split", {})
            split_enabled = split_config.get("enabled", True)

            if split_enabled:
                await self._send_split_message(
                    session, message, original_message, proactive_prompt_used
                )
            else:
                await self._send_single_message(session, message, proactive_prompt_used)

        except Exception as e:
            logger.error(f"âŒ å‘é€æ¶ˆæ¯æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            import traceback

            logger.error(f"å‘é€é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")

    async def _send_split_message(
        self,
        session: str,
        message: str,
        original_message: str,
        proactive_prompt_used: str = None,
    ):
        """å‘é€åˆ†å‰²åçš„æ¶ˆæ¯

        Args:
            session: ä¼šè¯ID
            message: å¾…åˆ†å‰²å’Œå‘é€çš„æ¶ˆæ¯
            original_message: åŸå§‹æ¶ˆæ¯
            proactive_prompt_used: æœ¬æ¬¡ä½¿ç”¨çš„ä¸»åŠ¨å¯¹è¯æç¤ºè¯
        """
        split_config = self.config.get("message_split", {})
        split_mode = split_config.get("mode", "backslash")

        # ç¡®å®šä½¿ç”¨çš„æ­£åˆ™è¡¨è¾¾å¼
        if split_mode == "custom":
            split_pattern = split_config.get("custom_pattern", "")
            if not split_pattern:
                logger.warning("customæ¨¡å¼ä¸‹æœªé…ç½®æ­£åˆ™è¡¨è¾¾å¼,ä½¿ç”¨é»˜è®¤backslashæ¨¡å¼")
                split_pattern = self.SPLIT_MODE_PATTERNS["backslash"]
                split_mode = "backslash"
        else:
            split_pattern = self.SPLIT_MODE_PATTERNS.get(
                split_mode, self.SPLIT_MODE_PATTERNS["backslash"]
            )

        try:
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²
            message_parts = re.split(split_pattern, message)
            message_parts = [part.strip() for part in message_parts if part.strip()]

            if len(message_parts) > 1:
                # åˆ†å‰²æˆå¤šä¸ªç‰‡æ®µ
                mode_display = (
                    f"{split_mode}æ¨¡å¼"
                    if split_mode != "custom"
                    else f"è‡ªå®šä¹‰æ¨¡å¼(/{split_pattern}/)"
                )
                logger.info(f"ğŸ“¨ ä½¿ç”¨{mode_display}åˆ†å‰²æ¶ˆæ¯,å…± {len(message_parts)} æ¡")

                delay_ms = split_config.get("delay_ms", 500)
                delay_seconds = delay_ms / 1000.0

                sent_count = 0
                for i, part in enumerate(message_parts, 1):
                    try:
                        message_chain = MessageChain().message(part)
                        success = await self.context.send_message(
                            session, message_chain
                        )

                        if success:
                            sent_count += 1
                            logger.debug(
                                f"  âœ… å·²å‘é€ç¬¬ {i}/{len(message_parts)} æ¡æ¶ˆæ¯"
                            )
                            if i < len(message_parts):
                                await asyncio.sleep(delay_seconds)
                        else:
                            logger.warning(
                                f"  âš ï¸ ç¬¬ {i}/{len(message_parts)} æ¡æ¶ˆæ¯å‘é€å¤±è´¥"
                            )

                    except Exception as part_error:
                        logger.error(
                            f"  âŒ å‘é€ç¬¬ {i}/{len(message_parts)} æ¡æ¶ˆæ¯æ—¶å‡ºé”™: {part_error}"
                        )

                if sent_count > 0:
                    self.user_info_manager.record_sent_time(session)
                    await self.conversation_manager.add_message_to_conversation_history(
                        session,
                        original_message,
                        proactive_prompt_used=proactive_prompt_used,
                        build_user_context_func=self.user_info_manager.build_user_context_for_proactive,
                    )
                    logger.info(
                        f"âœ… æˆåŠŸå‘é€ä¸»åŠ¨æ¶ˆæ¯({sent_count}/{len(message_parts)} æ¡)"
                    )
                else:
                    logger.warning("âš ï¸ æ‰€æœ‰æ¶ˆæ¯ç‰‡æ®µéƒ½å‘é€å¤±è´¥")
            else:
                # æ²¡æœ‰è¢«åˆ†å‰²
                await self._send_single_message(session, message, proactive_prompt_used)

        except re.error as e:
            logger.error(
                f"âŒ æ­£åˆ™è¡¨è¾¾å¼é”™è¯¯: {e}, æ¨¡å¼: {split_mode}, è¡¨è¾¾å¼: {split_pattern}"
            )
            logger.error("å°†ä½¿ç”¨åŸå§‹æ¶ˆæ¯,ä¸è¿›è¡Œåˆ†å‰²")
            await self._send_single_message(session, message, proactive_prompt_used)

    async def _send_single_message(
        self, session: str, message: str, proactive_prompt_used: str = None
    ):
        """å‘é€å•æ¡æ¶ˆæ¯

        Args:
            session: ä¼šè¯ID
            message: æ¶ˆæ¯å†…å®¹
            proactive_prompt_used: æœ¬æ¬¡ä½¿ç”¨çš„ä¸»åŠ¨å¯¹è¯æç¤ºè¯
        """
        message_chain = MessageChain().message(message)
        success = await self.context.send_message(session, message_chain)

        if success:
            self.user_info_manager.record_sent_time(session)
            await self.conversation_manager.add_message_to_conversation_history(
                session,
                message,
                proactive_prompt_used=proactive_prompt_used,
                build_user_context_func=self.user_info_manager.build_user_context_for_proactive,
            )
            logger.info("âœ… æˆåŠŸå‘é€ä¸»åŠ¨æ¶ˆæ¯")
        else:
            logger.warning("âš ï¸ ä¸»åŠ¨æ¶ˆæ¯å‘é€å¤±è´¥ï¼Œå¯èƒ½æ˜¯ä¼šè¯ä¸å­˜åœ¨æˆ–å¹³å°ä¸æ”¯æŒ")
